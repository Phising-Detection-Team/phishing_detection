import os
from pathlib import Path
from semantic_kernel.functions import kernel_function
from google import genai
from google.genai import types


class PromptLoader:
    """Utility class to load prompt templates from prompts.md file."""

    _prompts = None

    @classmethod
    def load_prompts(cls):
        """Load and parse prompts from prompts.md file."""
        if cls._prompts is not None:
            return cls._prompts

        prompts_file = Path(__file__).parent / 'prompts.md'
        with open(prompts_file, 'r', encoding='utf-8') as f:
            content = f.read()

        cls._prompts = {}

        # Parse judge prompts
        if '## Judge Agent Prompts' in content:
            judge_section = content[content.find('## Judge Agent Prompts'):]

            if '### System Prompt' in judge_section:
                start = judge_section.find('### System Prompt')
                end = judge_section.find('```', start + 20)
                next_section = judge_section.find('###', end)
                system_content = judge_section[start:next_section].split('```')[1].strip()
                cls._prompts['judge_system'] = system_content

            if '### Match Judgment Prompt' in judge_section:
                start = judge_section.find('### Match Judgment Prompt')
                end = judge_section.find('```', start + 30)
                next_section = judge_section.find('###', end)
                judgment_content = judge_section[start:next_section].split('```')[1].strip()
                cls._prompts['judge_judgment'] = judgment_content

            if '### Performance Analysis Prompt' in judge_section:
                start = judge_section.find('### Performance Analysis Prompt')
                end = judge_section.find('```', start + 35)
                next_section = judge_section.find('###', end)
                performance_content = judge_section[start:next_section].split('```')[1].strip()
                cls._prompts['judge_performance'] = performance_content

            if '### Progress Tracking Prompt' in judge_section:
                start = judge_section.find('### Progress Tracking Prompt')
                end = judge_section.find('```', start + 30)
                tracking_content = judge_section[start:].split('```')[1].strip()
                cls._prompts['judge_tracking'] = tracking_content

        return cls._prompts

    @classmethod
    def get_prompt(cls, prompt_name: str) -> str:
        """Get a specific prompt by name."""
        prompts = cls.load_prompts()
        return prompts.get(prompt_name, '')


class JudgeAgent:
    """
    Judge Agent: Evaluates the performance of both Generator and Detector agents.
    Uses Google Gemini to determine which agent "wins" and provides scores for improvement.
    """

    def __init__(self):
        """Initialize the Judge Agent with Gemini API."""
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY environment variable is required")

        self.client = genai.Client(api_key=api_key)
        self.model = 'models/gemini-2.5-flash'
        self.prompts = PromptLoader.load_prompts()

    @kernel_function(
        description="Evaluates both agents and determines the winner",
        name="judge_match"
    )
    async def judge_match(self, generated_email: str, detection_result: str) -> str:
        """
        Judge the match between generator and detector.

        Args:
            generated_email: The scam email generated by the generator agent
            detection_result: The detection result from the detector agent

        Returns:
            Judgment with scores and winner declaration
        """
        # Load prompt template and format with data
        prompt_template = PromptLoader.get_prompt('judge_judgment')
        prompt = prompt_template.format(
            generated_email=generated_email,
            detection_result=detection_result
        )

        # Call Gemini API using new SDK
        response = await self.client.aio.models.generate_content(
            model=self.model,
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0.7,
                max_output_tokens=2000
            )
        )

        return response.text

    @kernel_function(
        description="Provides detailed performance metrics for both agents",
        name="analyze_performance"
    )
    async def analyze_performance(self, generated_email: str, detection_result: str) -> str:
        """
        Provide detailed performance analysis for both agents.

        Args:
            generated_email: The scam email generated
            detection_result: The detection result

        Returns:
            Detailed performance metrics and analysis
        """
        # Load prompt template and format with data
        prompt_template = PromptLoader.get_prompt('judge_performance')
        prompt = prompt_template.format(
            generated_email=generated_email,
            detection_result=detection_result
        )

        # Call Gemini API using new SDK
        response = await self.client.aio.models.generate_content(
            model=self.model,
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0.7,
                max_output_tokens=4000
            )
        )

        return response.text

    @kernel_function(
        description="Tracks progress over multiple rounds and identifies trends",
        name="track_progress"
    )
    async def track_progress(self, match_history: str) -> str:
        """
        Analyze progress over multiple rounds.

        Args:
            match_history: History of previous matches

        Returns:
            Progress analysis and trends
        """
        # Load prompt template and format with data
        prompt_template = PromptLoader.get_prompt('judge_tracking')
        prompt = prompt_template.format(match_history=match_history)

        # Call Gemini API using new SDK
        response = await self.client.aio.models.generate_content(
            model=self.model,
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0.7,
                max_output_tokens=3000
            )
        )

        return response.text
